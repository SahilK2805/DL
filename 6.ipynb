{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9db506ea-0a45-45f6-a0dd-00b45dc8486a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMG_SIZE: (224, 224) BATCH_SIZE: 32 SEED: 42\n"
     ]
    }
   ],
   "source": [
    "# ---------------- Cell 1: Imports & defaults ----------------\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers, models, callbacks, optimizers\n",
    "import os\n",
    "\n",
    "# sensible defaults (override earlier cells if already defined)\n",
    "IMG_SIZE = globals().get(\"IMG_SIZE\", (224, 224))   # model input size\n",
    "BATCH_SIZE = globals().get(\"BATCH_SIZE\", 32)\n",
    "SEED = globals().get(\"SEED\", 42)\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "print(\"IMG_SIZE:\", IMG_SIZE, \"BATCH_SIZE:\", BATCH_SIZE, \"SEED:\", SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01f26302-c7e0-4733-a76a-6e37cd83ba0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes -> X_tr: (45000, 32, 32, 3) X_val: (5000, 32, 32, 3) X_test: (10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# ---------------- Cell 2: Load CIFAR-10 and create train/val/test tf.data pipelines ----------------\n",
    "(x_train_all, y_train_all), (x_test_all, y_test_all) = tf.keras.datasets.cifar10.load_data()\n",
    "y_train_all = y_train_all.flatten()\n",
    "y_test_all = y_test_all.flatten()\n",
    "\n",
    "# Stratified split: 90% train, 10% validation from training set\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    x_train_all, y_train_all, test_size=0.1, random_state=SEED, stratify=y_train_all\n",
    ")\n",
    "\n",
    "print(\"Shapes -> X_tr:\", X_tr.shape, \"X_val:\", X_val.shape, \"X_test:\", x_test_all.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1881f1bc-1d49-42cc-8469-7b2614045cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_ds element spec: (TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 10), dtype=tf.float32, name=None))\n",
      "val_ds   element spec: (TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 10), dtype=tf.float32, name=None))\n",
      "test_ds  element spec: (TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 10), dtype=tf.float32, name=None))\n"
     ]
    }
   ],
   "source": [
    "# ---------------- Cell 3: Preprocessing and tf.data datasets ----------------\n",
    "def preprocess_train(image, label):\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    image = tf.image.resize(image, IMG_SIZE)  # lazy resize per example\n",
    "    # Augmentations:\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_brightness(image, max_delta=0.08)\n",
    "    image.set_shape([IMG_SIZE[0], IMG_SIZE[1], 3])\n",
    "    label = tf.one_hot(label, NUM_CLASSES)\n",
    "    return image, label\n",
    "\n",
    "def preprocess_val(image, label):\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    image = tf.image.resize(image, IMG_SIZE)\n",
    "    image.set_shape([IMG_SIZE[0], IMG_SIZE[1], 3])\n",
    "    label = tf.one_hot(label, NUM_CLASSES)\n",
    "    return image, label\n",
    "\n",
    "# Create tf.data train & val pipelines\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_tr, y_tr))\n",
    "train_ds = train_ds.shuffle(buffer_size=5000, seed=SEED)\n",
    "train_ds = train_ds.map(preprocess_train, num_parallel_calls=AUTOTUNE)\n",
    "train_ds = train_ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "val_ds = val_ds.map(preprocess_val, num_parallel_calls=AUTOTUNE)\n",
    "val_ds = val_ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "\n",
    "# Also create a test dataset (no augmentation)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test_all, y_test_all))\n",
    "def preprocess_test(image, label):\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    image = tf.image.resize(image, IMG_SIZE)\n",
    "    image.set_shape([IMG_SIZE[0], IMG_SIZE[1], 3])\n",
    "    label = tf.one_hot(label, NUM_CLASSES)\n",
    "    return image, label\n",
    "test_ds = test_ds.map(preprocess_test, num_parallel_calls=AUTOTUNE).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "\n",
    "print(\"train_ds element spec:\", train_ds.element_spec)\n",
    "print(\"val_ds   element spec:\", val_ds.element_spec)\n",
    "print(\"test_ds  element spec:\", test_ds.element_spec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e28f12a-6d23-4317-a4b4-c3c048668300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built new MobileNetV2-based model (backbone frozen).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                         </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape                </span>â”ƒ<span style=\"font-weight: bold\">         Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ mobilenetv2_1.00_224 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)          â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooling2d             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)                â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)             â”‚                             â”‚                 â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)                â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">327,936</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,570</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)         â”‚               \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ mobilenetv2_1.00_224 (\u001b[38;5;33mFunctional\u001b[0m)    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)          â”‚       \u001b[38;5;34m2,257,984\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooling2d             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)                â”‚               \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)             â”‚                             â”‚                 â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (\u001b[38;5;33mDropout\u001b[0m)                    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)                â”‚               \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 â”‚         \u001b[38;5;34m327,936\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 â”‚               \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  â”‚           \u001b[38;5;34m2,570\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,588,490</span> (9.87 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,588,490\u001b[0m (9.87 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">330,506</span> (1.26 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m330,506\u001b[0m (1.26 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ---------------- Cell 4: Build or reuse 'model' (transfer learning) ----------------\n",
    "# If a model already exists in the notebook (e.g., you defined 'model' earlier), we will keep it.\n",
    "# Otherwise build a lightweight transfer learning model using MobileNetV2 backbone.\n",
    "\n",
    "if 'model' in globals():\n",
    "    print(\"Using existing model object found in workspace.\")\n",
    "else:\n",
    "    # Build model with pretrained mobile backbone (not include top)\n",
    "    base = tf.keras.applications.MobileNetV2(\n",
    "        input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3),\n",
    "        include_top=False,\n",
    "        weights='imagenet'\n",
    "    )\n",
    "    base.trainable = False  # freeze backbone for head training\n",
    "\n",
    "    inputs = layers.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n",
    "    x = base(inputs, training=False)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    outputs = layers.Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "\n",
    "    model = models.Model(inputs, outputs)\n",
    "    print(\"Built new MobileNetV2-based model (backbone frozen).\")\n",
    "\n",
    "# Compile\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=1e-3),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b79d2e0-9097-441b-b074-ea3948ff4db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Cell 5: Callbacks ----------------\n",
    "checkpoint_path = \"best_model_checkpoint.keras\"\n",
    "cb_list = [\n",
    "    callbacks.ModelCheckpoint(checkpoint_path, save_best_only=True, monitor='val_loss', mode='min'),\n",
    "    callbacks.EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True),\n",
    "    callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6920df-3465-4995-8d69-47f4987b7c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    }
   ],
   "source": [
    "# ---------------- Cell 6: Train classifier head (backbone frozen) ----------------\n",
    "EPOCHS_HEAD = 4  # tune as needed\n",
    "\n",
    "# ensure backbone is frozen for head training\n",
    "try:\n",
    "    base.trainable = False\n",
    "except NameError:\n",
    "    pass  # if base isn't defined because model was prebuilt differently\n",
    "\n",
    "history_head = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS_HEAD,\n",
    "    callbacks=cb_list,\n",
    "    verbose=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c28f00f-7fa1-492d-874f-b9e2ee5519a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Cell 7: Optional fine-tune (unfreeze some backbone layers) ----------------\n",
    "FINE_TUNE = True\n",
    "UNFREEZE_AT = -50  # number of layers from end to unfreeze (tweak as needed)\n",
    "\n",
    "if FINE_TUNE:\n",
    "    # Unfreeze from the last UNFREEZE_AT layers\n",
    "    try:\n",
    "        base.trainable = True\n",
    "        # Freeze earlier layers\n",
    "        for layer in base.layers[:UNFREEZE_AT]:\n",
    "            layer.trainable = False\n",
    "        for layer in base.layers[UNFREEZE_AT:]:\n",
    "            layer.trainable = True\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=optimizers.Adam(learning_rate=1e-4),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "        EPOCHS_FINE = 6\n",
    "        history_fine = model.fit(\n",
    "            train_ds,\n",
    "            validation_data=val_ds,\n",
    "            epochs=EPOCHS_FINE,\n",
    "            callbacks=cb_list,\n",
    "            verbose=2\n",
    "        )\n",
    "    except NameError:\n",
    "        print(\"Base model not available for fine-tuning; skip.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b156e8c-6cfe-4959-854e-fadcfff47ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Cell 8: Evaluate using val_ds (same preprocessing as training) ----------------\n",
    "val_loss, val_acc = model.evaluate(val_ds, verbose=2)\n",
    "print(f\"Validation loss: {val_loss:.4f}, Validation accuracy: {val_acc:.4f}\")\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_ds, verbose=2)\n",
    "print(f\"Test loss: {test_loss:.4f}, Test accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6035ca-f0d5-4f13-9b85-dd2c0550682c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Cell 9: Save model and prediction helper ----------------\n",
    "# Save as TensorFlow SavedModel format\n",
    "model_save_path = \"final_model_saved\"\n",
    "model.save(model_save_path, save_format=\"tf\")\n",
    "print(\"Saved model to\", model_save_path)\n",
    "\n",
    "# Single-image prediction helper\n",
    "import numpy as np\n",
    "\n",
    "def predict_single_image(img_array, model, class_names=None):\n",
    "    \"\"\"\n",
    "    img_array : numpy array, shape (H,W,3) in uint8 or float [0,255]\n",
    "    model : the trained tf.keras Model\n",
    "    class_names: optional list mapping label index to human name\n",
    "    \"\"\"\n",
    "    img = tf.cast(img_array, tf.float32) / 255.0\n",
    "    img = tf.image.resize(img, IMG_SIZE)\n",
    "    img = tf.expand_dims(img, axis=0)  # batch dim\n",
    "    probs = model.predict(img)\n",
    "    pred_idx = int(np.argmax(probs, axis=-1)[0])\n",
    "    confidence = float(np.max(probs))\n",
    "    label = class_names[pred_idx] if class_names is not None else pred_idx\n",
    "    return label, confidence, probs[0]\n",
    "\n",
    "# class names for CIFAR-10:\n",
    "class_names = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb194494-d778-4129-a5f9-4bfa1d57ab33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Cell 10: Demo prediction on first test image ----------------\n",
    "sample_img = x_test_all[0]          # original CIFAR test image (32x32)\n",
    "label, conf, probs = predict_single_image(sample_img, model, class_names)\n",
    "print(\"Predicted:\", label, \"Confidence:\", conf)\n",
    "print(\"Top probs:\", np.round(probs, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c5e35d-2c07-4b75-8d78-b469e4c05e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§  PART 1: What this experiment is about (in simple words)\n",
    "# This experiment shows how to perform Object Detection / Image Classification using Transfer Learning â€”\n",
    "# a method where we reuse a pre-trained CNN (like VGG16, ResNet, or MobileNet) that has already learned features from a large dataset (e.g., ImageNet).\n",
    "# Instead of training from scratch, we:\n",
    "# 1.\tLoad a pre-trained CNN model\n",
    "# 2.\tFreeze its early layers (they already know how to detect general features like edges, shapes)\n",
    "# 3.\tAdd new classifier layers on top for our specific dataset\n",
    "# 4.\tTrain only the new layers\n",
    "# 5.\tOptionally unfreeze deeper layers and fine-tune them for better accuracy.\n",
    "# ğŸ§© Think of it like taking a student who already knows how to recognize shapes and teaching them to recognize a new type of object faster â€” instead of starting from zero.\n",
    "# ________________________________________\n",
    "# ğŸ“˜ PART 2: Exam-Ready Notes\n",
    "# ğŸ§© Title\n",
    "# Object Detection using Transfer Learning of CNN Architectures\n",
    "# ________________________________________\n",
    "# ğŸ¯ Aim\n",
    "# To build an object detection/classification model using transfer learning with a pre-trained CNN,\n",
    "# by freezing early layers and training new classifier layers for the given dataset.\n",
    "# ________________________________________\n",
    "# ğŸ§  Theory (in simple words)\n",
    "# â€¢\tCNN (Convolutional Neural Network) models like VGG16, ResNet50, InceptionV3, and MobileNet are trained on massive datasets like ImageNet with millions of images.\n",
    "# â€¢\tThese networks learn to detect low-level features (edges, shapes, colors) and high-level features (objects, textures).\n",
    "# â€¢\tTransfer Learning uses these pre-learned features and adapts them to a smaller, specific dataset â€” saving time and improving performance.\n",
    "# â€¢\tSteps:\n",
    "# 1.\tLoad a pre-trained CNN (e.g., VGG16)\n",
    "# 2.\tFreeze the convolutional base layers\n",
    "# 3.\tAdd new Dense layers for classification\n",
    "# 4.\tTrain only these new layers\n",
    "# 5.\tOptionally unfreeze some deeper layers and fine-tune for accuracy\n",
    "# ğŸ“Š Result\n",
    "# â€¢\tThe transfer learning model successfully classifies images with high accuracy (usually >90% for small datasets).\n",
    "# â€¢\tTraining time is much shorter compared to building a CNN from scratch.\n",
    "# â€¢\tModel learns both general (edges, textures) and task-specific features.\n",
    "# ________________________________________\n",
    "# âœ… Conclusion\n",
    "# â€¢\tTransfer Learning reuses pre-trained CNN knowledge to efficiently solve new tasks.\n",
    "# â€¢\tFreezing lower layers preserves general visual features.\n",
    "# â€¢\tFine-tuning deeper layers improves model accuracy.\n",
    "# â€¢\tThis method is widely used in modern computer vision (e.g., image classification, object detection, medical imaging).\n",
    "# ________________________________________\n",
    "# ğŸ’¬ PART 3: Viva Questions (Simple Answers)\n",
    "# Question\tAnswer (Simple)\n",
    "# What is Transfer Learning?\tUsing a model pre-trained on one dataset for another similar task.\n",
    "# Why freeze the base layers?\tTo keep general visual features learned from large datasets.\n",
    "# Why add new layers?\tTo train on new, task-specific classes.\n",
    "# What dataset is the base model pre-trained on?\tImageNet (commonly used, with 1,000 classes).\n",
    "# What optimizer is used?\tAdam or SGD.\n",
    "# What happens if you unfreeze too many layers?\tThe model may overfit or forget earlier knowledge (catastrophic forgetting).\n",
    "# What is fine-tuning?\tAdjusting a few layers of a pre-trained model with a low learning rate.\n",
    "# Why use dropout?\tTo prevent overfitting during training.\n",
    "# What are examples of pre-trained models?\tVGG16, ResNet50, MobileNet, InceptionV3, EfficientNet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c90c891-4e40-46ca-a8b7-b6c9228645b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
